{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKMq7dp2W15Y"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KTVLipuQVdvh",
        "outputId": "780f4aa4-7168-4ac8-a938-9271100d0723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmWCBWxrBUB3"
      },
      "source": [
        "## 1. Генерирование русских имен при помощи RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUAk2yAxToEF",
        "outputId": "aa638fa9-41f3-4b0d-f060-7cc4476b5d2e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>авдокея</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>авдоким</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      name\n",
              "0  авдокея\n",
              "1  авдоким"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "names = pd.read_csv(\"./data/name_rus.txt\",\n",
        "            encoding=\"cp1251\", header=None, names=[\"name\"])\n",
        "names.name = names.name.str.lower().str.strip()\n",
        "names.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LKIIYfCToEG"
      },
      "source": [
        "Датасет: https://disk.yandex.ru/i/2yt18jHUgVEoIw\n",
        "\n",
        "1.1 На основе файла name_rus.txt создайте датасет.\n",
        "  * Учтите, что имена могут иметь различную длину\n",
        "  * Добавьте 4 специальных токена: \n",
        "    * `<PAD>` для дополнения последовательности до нужной длины;\n",
        "    * `<UNK>` для корректной обработки ранее не встречавшихся токенов;\n",
        "    * `<SOS>` для обозначения начала последовательности;\n",
        "    * `<EOS>` для обозначения конца последовательности.\n",
        "  * Преобразовывайте строку в последовательность индексов с учетом следующих замечаний:\n",
        "    * в начало последовательности добавьте токен `<SOS>`;\n",
        "    * в конец последовательности добавьте токен `<EOS>` и, при необходимости, несколько токенов `<PAD>`;\n",
        "  * `Dataset.__get_item__` возращает две последовательности: последовательность для обучения и правильный ответ. \n",
        "  \n",
        "  Пример:\n",
        "  ```\n",
        "  s = 'The cat sat on the mat'\n",
        "  # преобразуем в индексы\n",
        "  s_idx = [2, 5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  # получаем x и y (__getitem__)\n",
        "  x = [2, 5, 1, 2, 8, 4, 7, 3, 0]\n",
        "  y = [5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaTZSdFMToEG"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, names):\n",
        "        xs = names.iloc[:, 0]\n",
        "        self.max_seq_len = xs.str.len().max()\n",
        "        tokens = set()\n",
        "        for name in xs:\n",
        "            tokens.update(name)    \n",
        "        self.idx_to_token = dict(enumerate(tokens, 4))\n",
        "        self.idx_to_token[0] = \"<PAD>\"\n",
        "        self.idx_to_token[1] = \"<UNK>\"\n",
        "        self.idx_to_token[2] = \"<SOS>\"\n",
        "        self.idx_to_token[3] = \"<EOS>\"\n",
        "        self.token_to_idx = {token: idx for idx, token in self.idx_to_token.items()}\n",
        "        self.vocab_len = len(self.idx_to_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHNkCt1rToEH"
      },
      "outputs": [],
      "source": [
        "class RusNamesDataset(Dataset):\n",
        "    def __init__(self, X, vocab: Vocab):\n",
        "        self.X = X\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def vectorize(self, surname):\n",
        "        surname = surname[:vocab.max_seq_len]\n",
        "        surname_t = torch.zeros(self.vocab.max_seq_len+2).type(torch.long)\n",
        "        surname_t += self.vocab.token_to_idx[\"<PAD>\"]\n",
        "        for i, token in enumerate(surname, 1):\n",
        "            try:\n",
        "                surname_t[i] = self.vocab.token_to_idx[token]\n",
        "            except IndexError as e:\n",
        "                surname_t[i] = self.vocab.token_to_idx[\"<UNK>\"]\n",
        "        surname_t[0] = self.vocab.token_to_idx[\"<SOS>\"]\n",
        "        surname_t[-1] = self.vocab.token_to_idx[\"<EOS>\"]\n",
        "        return surname_t[:-1], surname_t[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.vectorize(self.X[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWA6DPZ9ToEH"
      },
      "outputs": [],
      "source": [
        "vocab = Vocab(names)\n",
        "dataset = RusNamesDataset(names.name.values, vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy52ciwJToEH"
      },
      "source": [
        "1.2 Создайте и обучите модель для генерации фамилии.\n",
        "\n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`;\n",
        "  * Используйте рекуррентные слои;\n",
        "  * Задача ставится как предсказание следующего токена в каждом примере из пакета для каждого момента времени. Т.е. в данный момент времени по текущей подстроке предсказывает следующий символ для данной строки (задача классификации);\n",
        "  * Примерная схема реализации метода `forward`:\n",
        "  ```\n",
        "    input_X: [batch_size x seq_len] -> nn.Embedding -> emb_X: [batch_size x seq_len x embedding_size]\n",
        "    emb_X: [batch_size x seq_len x embedding_size] -> nn.RNN -> output: [batch_size x seq_len x hidden_size] \n",
        "    output: [batch_size x seq_len x hidden_size] -> torch.Tensor.reshape -> output: [batch_size * seq_len x hidden_size]\n",
        "    output: [batch_size * seq_len x hidden_size] -> nn.Linear -> output: [batch_size * seq_len x vocab_size]\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCSrbHm3ToEI"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, vocab_len, embedding_dim, hidden_size, padding_idx,\n",
        "                 num_layers=1):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(padding_idx=padding_idx,\n",
        "            num_embeddings=vocab_len, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(num_layers=num_layers, batch_first=True,\n",
        "            input_size=embedding_dim, hidden_size=hidden_size)\n",
        "        self.fc = nn.Linear(in_features=hidden_size, out_features=vocab_len)\n",
        "    def forward(self, x, h=None):\n",
        "        x = self.embedding(x)\n",
        "        x, h = self.rnn(x, h)\n",
        "        x = self.fc(x)\n",
        "        return x, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By9voAJ2ToEI"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_len=vocab.vocab_len,\n",
        "    embedding_dim=32,\n",
        "    hidden_size=64,\n",
        "    padding_idx=vocab.token_to_idx[\"<PAD>\"],\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drA8sJbOToEI",
        "outputId": "44023d71-bac9-4ea2-e570-0178c73e80d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 --- loss [1.5506]\n",
            "#10 --- loss [0.9683]\n",
            "#20 --- loss [0.8864]\n",
            "#30 --- loss [0.8391]\n",
            "#40 --- loss [0.8010]\n",
            "#50 --- loss [0.7751]\n",
            "#60 --- loss [0.7506]\n",
            "#70 --- loss [0.7347]\n",
            "#80 --- loss [0.7202]\n",
            "#90 --- loss [0.7059]\n",
            "#100 --- loss [0.6986]\n"
          ]
        }
      ],
      "source": [
        "epoch_step = 10\n",
        "n_epochs = 101\n",
        "for epoch in range(n_epochs):\n",
        "    y_pred = torch.empty(0)\n",
        "    y_true = torch.empty(0, dtype=torch.long)\n",
        "    model.train()\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions, h = model(X_batch)\n",
        "        loss = criterion(predictions.transpose(1, -1), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            y_true = torch.cat((y_true, y_batch))\n",
        "            y_pred = torch.cat((y_pred, predictions))\n",
        "    with torch.no_grad():\n",
        "        train_loss = criterion(y_pred.transpose(1, -1), y_true).item()\n",
        "    if epoch % epoch_step == 0:\n",
        "        print(f\"#{epoch:3d} --- loss [{train_loss:.4f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "990obDBwCC7V"
      },
      "source": [
        "1.3 Напишите функцию, которая генерирует фамилию при помощи обученной модели:\n",
        "  * Построение начинается с последовательности единичной длины, состоящей из индекса токена `<SOS>`;\n",
        "  * Начальное скрытое состояние RNN `h_t = None`;\n",
        "  * В результате прогона последнего токена из построенной последовательности через модель получаете новое скрытое состояние `h_t` и распределение над всеми токенами из словаря;\n",
        "  * Выбираете 1 токен пропорционально вероятности и добавляете его в последовательность (можно воспользоваться `torch.multinomial`);\n",
        "  * Повторяете эти действия до тех пор, пока не сгенерирован токен `<EOS>` или не превышена максимальная длина последовательности.\n",
        "\n",
        "При обучении каждые `k` эпох генерируйте несколько фамилий и выводите их на экран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs3pHP17ToEJ"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, vocab, embedding_dim, hidden_size, num_layers=1):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(padding_idx=vocab.token_to_idx[\"<PAD>\"],\n",
        "            num_embeddings=vocab.vocab_len, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(num_layers=num_layers, batch_first=True,\n",
        "            input_size=embedding_dim, hidden_size=hidden_size)\n",
        "        self.fc = nn.Linear(\n",
        "            in_features=hidden_size, out_features=vocab.vocab_len)\n",
        "    def forward(self, x, h=None):\n",
        "        x = self.embedding(x)\n",
        "        x, h = self.rnn(x, h)\n",
        "        x = self.fc(x)\n",
        "        return x, h\n",
        "    \n",
        "    def random_word(self):\n",
        "        h = None\n",
        "        word = []\n",
        "        token = torch.tensor([[vocab.token_to_idx[\"<SOS>\"]]])\n",
        "        for _ in range(self.vocab.max_seq_len):\n",
        "            out, h = self(token, h)\n",
        "            out_random = out.squeeze().softmax(-1).multinomial(1)\n",
        "            letter = self.vocab.idx_to_token[out_random.item()]\n",
        "            if letter == \"<EOS>\":\n",
        "                break\n",
        "            word.append(letter)\n",
        "            token = out_random.view(1, 1)\n",
        "        clear = [letter for letter in word if letter not in [\n",
        "            \"<PAD>\",\"<EOS>\",\"<SOS>\",\"<UNK>\"]]\n",
        "        generated = \"\".join(word).capitalize()\n",
        "        clear = \"\".join(clear).capitalize()\n",
        "        return generated, clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ddcpnNtToEJ"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab=vocab,\n",
        "    embedding_dim=32,\n",
        "    hidden_size=64,\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4dXXnbDToEJ",
        "outputId": "f7319d22-44e6-4423-8cfc-3c0370f2724f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#  0 --- loss [2.2938]\n",
            "Алтнфб, Алреная, Лиднун, Дфиошиа, Пфрорю, Внйдета, Лчкеваналд, Асдиу, Виафатх, Вмиеаыаа\n",
            "# 10 --- loss [0.9925]\n",
            "Фэлюха, Алонтейын, Евося, Линя, Митуша, Ювяй, Инксилья, Трскавр, Веля, Едоша\n",
            "# 20 --- loss [0.9093]\n",
            "Корьай, Видора, Гарий, Леяра, Леманьич, Коля, Пегая, Падуня, Селья, Парося\n",
            "# 30 --- loss [0.8589]\n",
            "Вермюша, Тома, Валюша, Гетуся, Веля, Темалин, Кольона, Верктушя, Гельюшка, Настадка\n",
            "# 40 --- loss [0.8169]\n",
            "Иросеша, Нюхаиктай, Доря, Петюра, Апа, Мулиан, Ваислешка, Леда, Варюня, Гася\n",
            "# 50 --- loss [0.7931]\n",
            "Кита, Кирина, Никуля, Вируха, Паврюша, Филюша, Бероша, Варсич, Римуся, Бенюня\n",
            "# 60 --- loss [0.7668]\n",
            "Еждя, Диктуша, Нисент, Елюша, Юратюта, Мальб, Нюта, Панюшка, Владиска, Артонина\n",
            "# 70 --- loss [0.7522]\n",
            "Маруха, Павра, Алене, Нидилианка, Емилинка, Ситефмианка, Анатима, Лодя, Лаврена, Тюта\n",
            "# 80 --- loss [0.7333]\n",
            "Васяна, Ирася, Авдоня, Миняша, Петря, Лизанель, Васяша, Дуся, Маналя, Петря\n",
            "# 90 --- loss [0.7196]\n",
            "Антоныч, Коля, Докитка, Дониска, Катанодыч, Максин, Лежаша, Филия, Емаша, Кутуся\n",
            "#100 --- loss [0.7083]\n",
            "Вася, Эллуня, Илуша, Волюха, Алидуся, Дарья, Павлуня, Павилий, Нилка, Марианка\n"
          ]
        }
      ],
      "source": [
        "epoch_step = 10\n",
        "n_epochs = 101\n",
        "n_words = 10\n",
        "for epoch in range(n_epochs):\n",
        "    y_pred = torch.empty(0)\n",
        "    y_true = torch.empty(0, dtype=torch.long)\n",
        "    model.train()\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions, h = model(X_batch)\n",
        "        loss = criterion(predictions.transpose(1, -1), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            y_true = torch.cat((y_true, y_batch))\n",
        "            y_pred = torch.cat((y_pred, predictions))\n",
        "    with torch.no_grad():\n",
        "        train_loss = criterion(y_pred.transpose(1, -1), y_true).item()\n",
        "    if epoch % epoch_step == 0:\n",
        "        print(f\"#{epoch:3d} --- loss [{train_loss:.4f}]\")\n",
        "        words=[]\n",
        "        for _ in range(n_words):\n",
        "            _, clear = model.random_word()\n",
        "            words.append(clear)\n",
        "        print(\", \".join(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJf5iaA2fOTM"
      },
      "source": [
        "## 2. Генерирование текста при помощи RNN\n",
        "\n",
        "2.1 Скачайте из интернета какое-нибудь художественное произведение\n",
        "  * Выбирайте достаточно крупное произведение, чтобы модель лучше обучалась;\n",
        "\n",
        "2.2 На основе выбранного произведения создайте датасет. \n",
        "\n",
        "Отличия от задачи 1:\n",
        "  * Токены <SOS>, `<EOS>` и `<UNK>` можно не добавлять;\n",
        "  * При создании датасета текст необходимо предварительно разбить на части. Выберите желаемую длину последовательности `seq_len` и разбейте текст на построки длины `seq_len` (можно без перекрытия, можно с небольшим перекрытием)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN0hquhgT6Ld",
        "outputId": "a51d2e29-975c-4581-fa25-6ccce8215afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'wap2.txt', 'wap4.txt', 'wap1.txt', 'wap3.txt', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L19bOxMToEK"
      },
      "outputs": [],
      "source": [
        "text = \"\"\n",
        "for i in range(1, 5):\n",
        "    with open(f\"./wap{i}.txt\", 'r', encoding='cp1251') as f:\n",
        "        text += f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDSkREHDToEK"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, text):\n",
        "        tokens = set(text) \n",
        "        self.idx_to_token = dict(enumerate(tokens, 2))\n",
        "        self.idx_to_token[0] = \"<PAD>\"\n",
        "        self.idx_to_token[1] = \"<SOS>\"\n",
        "        self.token_to_idx = {token: idx for idx, token in self.idx_to_token.items()}\n",
        "        self.vocab_len = len(self.idx_to_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47LJrdmnToEK"
      },
      "outputs": [],
      "source": [
        "class BookDataset(Dataset):\n",
        "    def __init__(self, text, vocab: Vocab, seq_len=128, shift=None):\n",
        "        self.seq_len = seq_len\n",
        "        self.shift = shift\n",
        "        self.X = self.__text_split(text)\n",
        "        self.vocab = vocab\n",
        "        \n",
        "    def __text_split(self, text):\n",
        "        if self.shift is None: \n",
        "            self.shift = self.seq_len // 4\n",
        "        start = 0\n",
        "        X = []\n",
        "        while start < len(text):\n",
        "            X.append(text[start:start+self.seq_len])\n",
        "            start += self.shift\n",
        "        return np.array(X)\n",
        "    \n",
        "    def vectorize(self, fragment):\n",
        "        fragment_t = torch.zeros(self.seq_len+1).long()\n",
        "        fragment_t += self.vocab.token_to_idx[\"<PAD>\"]\n",
        "        for i, token in enumerate(fragment, 1):\n",
        "            fragment_t[i] = self.vocab.token_to_idx[token]\n",
        "        fragment_t[0] = self.vocab.token_to_idx[\"<SOS>\"]\n",
        "        return fragment_t[:-1], fragment_t[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.vectorize(self.X[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B7n_JhjToEK"
      },
      "outputs": [],
      "source": [
        "vocab = Vocab(text)\n",
        "dataset = BookDataset(text, vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W2bIBcAToEK"
      },
      "source": [
        "2.3 Создайте и обучите модель для генерации текста\n",
        "  * Задача ставится точно так же как в 1.2;\n",
        "  * При необходимости можете применить:\n",
        "    * двухуровневые рекуррентные слои (`num_layers`=2)\n",
        "    * [обрезку градиентов](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
        "\n",
        "2.4 Напишите функцию, которая генерирует фрагмент текста при помощи обученной модели\n",
        "  * Процесс генерации начинается с небольшого фрагмента текста `prime`, выбранного вами (1-2 слова) \n",
        "  * Сначала вы пропускаете через модель токены из `prime` и генерируете на их основе скрытое состояние рекуррентного слоя `h_t`;\n",
        "  * После этого вы генерируете строку нужной длины аналогично 1.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNzI8lc5ToEL"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, vocab, embedding_dim, hidden_size, num_layers=1):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(padding_idx=vocab.token_to_idx[\"<PAD>\"],\n",
        "            num_embeddings=vocab.vocab_len, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.LSTM(num_layers=num_layers, batch_first=True,\n",
        "            input_size=embedding_dim, hidden_size=hidden_size)\n",
        "        self.fc = nn.Linear(\n",
        "            in_features=hidden_size, out_features=vocab.vocab_len)\n",
        "        \n",
        "    def forward(self, x, hc=None):\n",
        "        x = self.embedding(x)\n",
        "        x, hc = self.rnn(x, hc)\n",
        "        x = self.fc(x)\n",
        "        return x, hc\n",
        "    \n",
        "    def gen_fragment(self, start_with, length=256):\n",
        "        hc = None\n",
        "        word = []\n",
        "        token = torch.tensor([[vocab.token_to_idx[\"<SOS>\"]]]).to(device)\n",
        "        for token__ in start_with:\n",
        "            out, hc = self(token, hc)\n",
        "            token = torch.tensor(\n",
        "                [[self.vocab.token_to_idx[token__]]],\n",
        "                dtype=torch.long\n",
        "                ).to(device)\n",
        "\n",
        "        for _ in range(length):\n",
        "            token = token.to(device)\n",
        "            out, hc = self(token, hc)\n",
        "            out_random = out.squeeze().softmax(-1).multinomial(1)\n",
        "            letter = self.vocab.idx_to_token[out_random.item()]\n",
        "            word.append(letter)\n",
        "            token = out_random.view(1, 1)\n",
        "        clear = [letter for letter in word if letter not in [\n",
        "            \"<PAD>\",\"<EOS>\",\"<SOS>\",\"<UNK>\"]]\n",
        "        generated = \"\".join(word).capitalize()\n",
        "        clear = \"\".join(clear).capitalize()\n",
        "        return generated, clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgfvOLteToEL"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab=vocab,\n",
        "    embedding_dim=32,\n",
        "    hidden_size=64,\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNMyuQi5ToEL",
        "outputId": "33568b3f-a331-4fdd-a653-407140309a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#  0 --- loss [2.4596]\n",
            "Щенстистнои как небы быднох, то престаке в к сказиника сьмя, итостарь и выхравни святстя. аплием этом содидичестлек на увиствая схотоврей ни изгралия. кок кобожмоми, дотое потрышать коствение. и стредшим у навожее семы и во таженьюе мосзатичаменных пось то\n",
            "#  1 --- loss [2.0512]\n",
            "; друга гриводночсе он седерашно начам проничажанатвает фная, замижении прободния дапера, предошитносо, придильсе его добвула не пошму (не доперживычьим кака... что и еравала изаже поте свете сапраштся. одивлает илик, и принове неми и тозмойках ду тата ник\n",
            "#  2 --- loss [1.9234]\n",
            "Коном. во пребедит, что бормию, поратии иболе прожадствия егообстикного сводехста свошля к человек?) наташи, постобитьком, бе косназовина, только чувстви, сердать отстан, с времил его в мака то вляпени присобититься водочно жины сао, на как быти, подама ра\n",
            "#  3 --- loss [1.8573]\n",
            "Нный толполосавших постечения извывался пьершения солдатся можение про, но он, - стакантов застредствемандующаяли человени. но человека бочели и местного произния севда бы пветилась, ноча есть страдали большевние мозяны представляя - тотя покажет совщестим\n",
            "#  4 --- loss [1.8168]\n",
            "Нь и застарьихий и расспретовучает подолжа семелческий польмами, сводямо, что она тьет, неожинать стрибият и неот наездащи страз, а, ежели знаханным прещитая слупка, улыбками, иртив обходув петывающими в деовенькому, я есть проничал тется свободы) много, п\n",
            "#  5 --- loss [1.7886]\n",
            "Сь к мне сам. стерманную сылатыт и одном ду общих, но обходинь они давроменяну, на менцу для s9 wet mastonde, bole da lacolles hour мании допрям ебы привитии хочет. из общей добов на, человь пону, показая как бы тапался с молча изящей значений, - прошваю. \n",
            "#  6 --- loss [1.7676]\n",
            "Ньким греданий, так дичкой бы в представлятить приводон присубесту, что немо историисков, состоили состиненно о безамитей, просновник... правилось только тридени. посладеняли истричественным от лугмины, выстакай людился и, все их абокоротельбареи в страушу\n",
            "#  7 --- loss [1.7512]\n",
            "Сь они в неихочитенесь уражили и состорожно андмю в талять живил остапанеение уеждеть непроев происчинать, - что сеодная причинию, последятся влакать веторы один свои чего и человекани; находом добой, при влятывают эти и а причкасти собода то ешились, не п\n",
            "#  8 --- loss [1.7378]\n",
            ", и какибство. доводланно и другой в этого и умень раззамо члюда вочении в оккучнак? часто для челося жизни, ими зановии рукимости. истобы теперь: мы изредеть и неизнакидью марью на колосом один можсторила, он человерить дронный может, чему сидперь этим, ч\n",
            "#  9 --- loss [1.7270]\n",
            "Нических приизыты другими безать в сужненя по признаемые некнои это нутоваться свостановились цель въейшему тень на долщев, которов необвами изрый. куторились в своими массо, которого вотсамо и восходи на крустие французов, отревина застугам. в свииции меж\n",
            "# 10 --- loss [1.7178]\n",
            "Во; ни, и боличным него оставь неокоторования, чтобы имения, бы было по опусшая назочи занимую сказывающих толкот молит.\n",
            "v.\n",
            "     в это нечивыи, и простилая ведеть доброде своий арми-точно дего свотстым, об ямпачал тобычим блепватопному заговочитенья. - и п\n",
            "# 11 --- loss [1.7100]\n",
            ", только силачать она дай котором него чего мениться, приказалы ующив; большув, давной, этих торшнокое\", свое назад свозна мирый черу накрии перечью, наставили в этих навиде человь нивносто, то нисмоторое закыв своислестного причпу история к кашимся она пр\n",
            "# 12 --- loss [1.7031]\n",
            "Ньками нео-то силаем\", миночно была другом для простанах твочно. и чем улыбку, свою наполеони, с него, где врикает. - тогу, точно. немое. если сиятером, что с ушела иль. имучиичественных всей его признались сопрашия одное бывает возвозил старожидае воплост\n",
            "# 13 --- loss [1.6969]\n",
            "Нсем моста же графиничами, но о только одный теперь вой. - истории, черовательную чувства. - в собяты, поклама жизного на месность по свобода, но причможе причинтов, цимо церкивает бы поводили остарие собою воправило сводется начего слятее словом надотност\n",
            "# 14 --- loss [1.6914]\n",
            "; домоба и к слезолостность оподани наред как бы стара, болистркую парии докабостлив выбока хорило только придостоя, никой необходих, попредскидно его закутай. кута бож из усмотрел поразоватинную историе мномакрит власти сидел доказая малостии и намедиих п\n",
            "# 15 --- loss [1.6865]\n",
            " так которого значильники, с во для лесомну; но в вся люди едему было, он навица сосшедутся хорошенье отущик, мы найтечии бивании и которые, но дня не бытоко монцко бы ресимизвенновами, которые вывесту ему необхадя околабы, так же необмые оподгерами, упреж\n",
            "# 16 --- loss [1.6821]\n",
            "Нький началась.\n",
            "     - а что в замернать сирают знакой хотери, общество., что был прилице неизгоряция молототечиной в представременпых он условившие сторониными набсть из пети причин нахочел времями, и покадуучения, так ного не понимать и это нашему, - при\n",
            "# 17 --- loss [1.6781]\n",
            "Нках, подама недочени необходя; слезу. сам к дею сруждывали на стреобытие не зай окров; молпочною и устерить: вchondrarigs, me sau de l'apoutint, это созывает находищей постоп и избукем намечи. я наконичаются то, как большей ннова, вограть онимо утему и ко\n",
            "# 18 --- loss [1.6745]\n",
            " историю ей? нам прибарь за тороплание, которое соднино и как на зеложит, так коргло придочено человека, в нее молча каком опреебести, паребиет сах возвражая вото не словок, живлению кажется дипление, как мога мысля состояли естьства, подрогия шкаши, и от \n",
            "# 19 --- loss [1.6713]\n",
            "Ньким йствовался на навсление. мне. - вот частром примозадь, которому и так я себе. и так жизни, греноса денисоватя длинате в, и едина слова была отпитом бы годова лезненно мирая проехать у мнем иникона бо, о прюблюда, может мессочной быть понять набудь за\n",
            "# 20 --- loss [1.6683]\n",
            " созжающего стела триконасменно жизни времясь сражения. он полачали услыйских всеминого подним для произвозиции; насму своим человека напловях и может треполодник-е бывае лисша озоньком межчет один между этим ваш и дела франаться повзодление ее проходие ли\n"
          ]
        }
      ],
      "source": [
        "epoch_step = 1\n",
        "n_epochs = 21\n",
        "n_words = 10\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0\n",
        "    k = 0\n",
        "    model.train()\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions, h = model(X_batch)\n",
        "        loss = criterion(predictions.transpose(1, -1), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        k += 1\n",
        "    epoch_loss = epoch_loss / k\n",
        "    if epoch % epoch_step == 0:\n",
        "        print(f\"#{epoch:3d} --- loss [{epoch_loss:.4f}]\")\n",
        "        _, clear = model.gen_fragment(start_with=\"В начале\")\n",
        "        print(\"\".join(clear))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3LOjBglToEL",
        "outputId": "a68949b5-c899-4c5b-950e-c53a6fac9668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 21 --- loss [1.6655]\n",
            " о собы в события, облама нам сосмычаем жизни, в самое оставлея миромя насказах свободание поряки, которого вадчей монищай, бостасатся начинатие постренноманду, который черезючно законаным данетина: х'ив, что которого напдановники уможно, стоял, спросани; \n",
            "# 22 --- loss [1.6630]\n",
            "М, позамернуют полепанию заей всемимость; нарочит пьера, движение, не было, которых себе, оструется. - франции подумая кучи силье о числом совершенного фидом видел на это было никогда ни в воль по дочеревился не стренно все блюден коренье находям, хорошо) \n",
            "# 23 --- loss [1.6607]\n",
            ". (сноским, он дело отпоражка, необыча, свой, ностветрекой известно держения, говорит, мходействии взялых обрачаем, если и пьера, понявший жится межа только от невнишал (обрадный именять насмейчества этого модшей быть в то, - переп, и событий, законии напо\n",
            "# 24 --- loss [1.6586]\n",
            ". встам. воя моя наступии и восточно тор, что они и которая начина и смотреть в собою. а он от танят так же движен наполеонакоками ты человека, как болеродовать подудета, понимаю, пережали и након интречите, и отчавных лек всеменному нам под извенесечи, вы\n",
            "# 25 --- loss [1.6565]\n",
            "Пника и тот, не полочности! подсык. кто было от убичения мидуровну в ними, понущайствия светце. николай вошели шла одина. радовение. что сильнею сверным ему от арминей наконечный секазать и было еще причинаетского базята и с навесту масонцачиван эти раз те\n",
            "# 26 --- loss [1.6545]\n",
            "Нье читального руком не спераз и собей на обис всем дерепка, челотю одинакев должны страшнию бы нах, же квесто надом, как бы он нои нани окти сожде, очевидить назадывно сказах народно тех только слабо. напозичин и известное должну отцы в армии новке и прис\n",
            "# 27 --- loss [1.6527]\n",
            ": образывая с водругими только едине напущать знавшая. morscont qui dis un folie les! sif никия несколько не освета, что изможеньшают, были дружают, от лицу. вассовство и все свершен. что не отном, в людей; что ты отруждения не дать дела меня.\n",
            "xviii.\n",
            "     \n",
            "# 28 --- loss [1.6509]\n",
            "Нной говории укучит ноча денисов осторы его неизвестная народа раз теперь самое отстании. разжам предку, - понявший смотря. и есть причиние служать. nepes. cajrre. [125] - смела куками предказ есть умгонию темком, подвинательнакия выбоковый изторону к быть\n",
            "# 29 --- loss [1.6493]\n",
            "Нное этих их станую услезовата монится, постою эти для так других бы из только и потом человека, необходимы заметив сле самерте есть новое назадов, необходимости наблюдении что безресланствовуя ничего нельзящее кряже стоятняе, блаже с наурействикании с пье\n",
            "# 30 --- loss [1.6477]\n",
            " ибмиство и от бедни улы им назах, что звездо. лицо и оказываем. - это находитских много блески бы сущих, тоих три вместя накону политетнома как брадлен и становию дуты. ест, в с нескизк алегодияз раз оподит надоскаи, свобудают), так жие житету молопос, и \n",
            "# 31 --- loss [1.6462]\n",
            "Нной слова это какой-то вошел нашем понях, моде быть правдала жишей, дали ты законтуии на только мы слова законерно оставляя молодого еще очеверни, той земли говорить из. я причатностии указил наем. чак под поквет быть одних затимаем - мойно, сосадители мо\n",
            "# 32 --- loss [1.6447]\n",
            "Нскум хотту знал никот в собоел зводался и двгах закрывает низического засмий всегда зарыки, и тоже чтобы постал трикто надобног быть, что не без ратся отношу. признаи уложке а наздит ты году, первую решаю; уже николеньких в элезался трежавра во, но всех у\n",
            "# 33 --- loss [1.6434]\n",
            "Ны, и неставром быть, что без злагомить, в тебе нынузьбу котором улогнойчеством путь и мне, то тряды, приами, поредка что по-опершает иплия жизни мы бысшизве от событ и и прать верее доплято людех тить и моднемая и всех из, защит, как бы проса свестно, мен\n",
            "# 34 --- loss [1.6421]\n",
            "Нии слова и тысте влого в этох так осневатыку, - не не никот мизрожень... и побира, что друга увидым принившести непреятся сождении привидитью, чевис нем люде. гредающего приказания к солдатские вышели на собой образия: царту, что смершания, то напушу, и к\n",
            "# 35 --- loss [1.6409]\n",
            " персам дело о ногероса и, ногами водомот знают только так как почему была. в это наконени, пелосно прямшем куда умнот, отношения двери, обещавной знарожительным с тот завидав остановию в сожденным неизрядки, не минять своимум поправляется говорил ничийств\n",
            "# 36 --- loss [1.6398]\n",
            "Никой про, кренный всем многость простурнем необходимость из комнат и что-то ясновольность и сам всякой ображается выдъед отвозуть сама люди масым услый ему понимать быть, как или лица, что стременивая, ровию друг? укоты необходимости, костоенствение, боль\n",
            "# 37 --- loss [1.6386]\n",
            "Сь разыно. встали. а печитеннее играртом, даже произврытие решит всех ты причинствищим на горыт бы войны вззьзывает душе на миром добра. но и движенио же блязь с дела жизни обычный, ро на состагит, слуду находим. событ вущая рукое в знающей никакого что ме\n",
            "# 38 --- loss [1.6375]\n",
            "Нии, тяжелодие возводления. все знай человечи. все они назвен говорить их поступухра истой и несчасть самых вознавегожали на полным народ, ктооворила может призвестным и его он всех которое теперно в которым, против своими тихоотемного его преждостри, как \n",
            "# 39 --- loss [1.6364]\n",
            "М сожает на и т'изнитовательную свердом. отадие заплачала только поемью. наполезнь должны мней пораживаю известную остановила к вставая и пени, ослин демученом то, как он указывала барбы еще францустнакий и пьер, о окан молодина тая хочит носельшемы наконе\n",
            "# 40 --- loss [1.6354]\n",
            "С которого вам их времнения с начать именности насступнуть не мать деобразие она ище сели в толмыхий знай; но необыкана, часта никогда, они не замярик, ощения выижел потом умять, граф! снагарка, разучные смощни, и воль величествия сменьшей мишо. тот колобу\n"
          ]
        }
      ],
      "source": [
        "epoch_step = 1\n",
        "for epoch in range(21, 41):\n",
        "    epoch_loss = 0\n",
        "    k = 0\n",
        "    model.train()\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions, h = model(X_batch)\n",
        "        loss = criterion(predictions.transpose(1, -1), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        k += 1\n",
        "    epoch_loss = epoch_loss / k\n",
        "    if epoch % epoch_step == 0:\n",
        "        print(f\"#{epoch:3d} --- loss [{epoch_loss:.4f}]\")\n",
        "        _, clear = model.gen_fragment(start_with=\"В начале\")\n",
        "        print(\"\".join(clear))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSCjU-nLndld"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}