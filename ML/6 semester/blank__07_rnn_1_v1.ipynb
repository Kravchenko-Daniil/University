{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKMq7dp2W15Y"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# nltk.download('punkt')\n",
        "import numpy as np\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBkqaK5bawXN"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm-QilGISxkt"
      },
      "source": [
        "## 1. Классификация фамилий (RNN)\n",
        "\n",
        "Датасет: https://disk.yandex.ru/d/frNchuaBQVLxyA?w=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfefDMRw_Pot",
        "outputId": "5121a8e8-4c18-4d75-e884-a3f17963333a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>surname</th>\n",
              "      <th>nationality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>woodford</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>coté</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    surname nationality\n",
              "0  woodford     English\n",
              "1      coté      French"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "surnames = pd.read_csv(\"./data/surnames.csv\")\n",
        "surnames[\"surname\"] = surnames[\"surname\"].str.lower()\n",
        "\n",
        "encoder = LabelEncoder().fit(surnames[\"nationality\"])\n",
        "y = encoder.transform(surnames[\"nationality\"])\n",
        "surnames.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pILoL96f_Pou"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, data):\n",
        "        self.max_seq_len = data.str.len().max()\n",
        "        self.tokens = set()\n",
        "        for item in data:\n",
        "            self.tokens.update(item)\n",
        "        self.idx_to_token = dict(enumerate(self.tokens, 1))\n",
        "        self.idx_to_token[0] = \"<PAD>\"\n",
        "        self.token_to_idx = {token: idx for idx, token in self.idx_to_token.items()}\n",
        "        self.vocab_len = len(self.idx_to_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xCrj2ZW_Pou"
      },
      "outputs": [],
      "source": [
        "class SurnamesDataset(Dataset):\n",
        "    def __init__(self, X, y, vocab: Vocab):\n",
        "        self.X = X\n",
        "        self.y = torch.LongTensor(y)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def vectorize(self, surname):\n",
        "        '''Генерирует представление фамилии surname в при помощи бинарного кодирования (см. 1.2)'''\n",
        "        surname_t = torch.zeros(self.vocab.max_seq_len).type(torch.long)\n",
        "        for i, token in enumerate(surname):\n",
        "            surname_t[i] = self.vocab.token_to_idx[token]\n",
        "        return surname_t\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.vectorize(self.X.iloc[idx]), self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODxR81O-_Pov"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(surnames.surname, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jcd7FHD_Pov"
      },
      "outputs": [],
      "source": [
        "vocab = Vocab(surnames[\"surname\"])\n",
        "trainset = SurnamesDataset(X_train, y_train, vocab)\n",
        "testset = SurnamesDataset(X_test, y_test, vocab)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=32)\n",
        "testloader = DataLoader(testset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdPr92i6k-If"
      },
      "source": [
        "1.1 Используя класс `nn.RNNCell` (абстракцию для отдельного временного шага RNN), реализуйте простейшую рекуррентную сеть Элмана в виде класса `RNN`. Используя созданный класс `RNN`, решите задачу классификации фамилий. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir6UUkl6l4tp"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnncell = nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
        "\n",
        "    def forward(self, x, h=None):\n",
        "        '''\n",
        "        x.shape = (batch_size, seq_len, feature_size) - тензор входных данных\n",
        "        h.shape = (batch_size, hidden_size) - тензор со скрытым состоянием RNN\n",
        "        '''\n",
        "        seq_len = x.size(1)\n",
        "        batch_size = x.size(0)\n",
        "        # инициализация тензора скрытых состояний\n",
        "        if not h:\n",
        "            h = torch.zeros(batch_size, self.hidden_size)\n",
        "        hs = torch.zeros(batch_size, seq_len, self.hidden_size)\n",
        "\n",
        "        # проход по каждому элементу последовательностей s в батче и обновление скрытого состояния\n",
        "        # h = RNNCell(s_t, h)\n",
        "        for i in range(seq_len):\n",
        "            h = self.rnncell(x[:,i,:], h)\n",
        "            hs[:,i,:] = h\n",
        "        # вернуть тензор всех наблюдавшихся скрытых состояний размера (batch_size, seq_len, hidden_size)\n",
        "        #и тензор скрытых состояний в последний момент времени\n",
        "        return h, hs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2WDDR_R_Pow"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocab_len, pad_idx, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.rnncell = RNN(input_size, hidden_size)\n",
        "        self.emb = nn.Embedding(\n",
        "            num_embeddings=vocab_len,\n",
        "            embedding_dim=input_size,\n",
        "            padding_idx=pad_idx\n",
        "        )\n",
        "        self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x, hs = self.rnncell(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYQEaF_d4erW"
      },
      "outputs": [],
      "source": [
        "num_classes = len(np.unique(y))\n",
        "\n",
        "model = MyModel(\n",
        "    input_size=64,\n",
        "    hidden_size=16,\n",
        "    vocab_len=vocab.vocab_len,\n",
        "    pad_idx=0,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-b_dAQU_Pox"
      },
      "outputs": [],
      "source": [
        "def model_train(model, optimizer, criterion, train_loader, test_loader, n_epochs=10, epoch_step=5):\n",
        "    for epoch in range(n_epochs):\n",
        "        y_pred = torch.empty(0)\n",
        "        y_true = torch.empty(0, dtype=torch.long)\n",
        "        model.train()\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(X_batch)\n",
        "            loss = criterion(predictions, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            with torch.no_grad():\n",
        "                y_true = torch.cat((y_true, y_batch))\n",
        "                y_pred = torch.cat((y_pred, predictions))\n",
        "        with torch.no_grad():\n",
        "            train_acc = accuracy_score(y_true, y_pred.argmax(dim=1)).item()\n",
        "            train_loss = criterion(y_pred, y_true).item()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = torch.empty(0)\n",
        "            y_true = torch.empty(0, dtype=torch.long)\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                predictions = model(X_batch)\n",
        "                y_true = torch.cat((y_true, y_batch))\n",
        "                y_pred = torch.cat((y_pred, predictions))\n",
        "            val_acc = accuracy_score(y_true, y_pred.argmax(dim=1)).item()\n",
        "            val_loss = criterion(y_pred, y_true).item()\n",
        "            \n",
        "            if epoch % epoch_step == 0:\n",
        "                print(f'#{epoch} Training loss: {train_loss:.4f} training_acc:\\\n",
        " {train_acc:.4f} val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "    return y_true, y_pred.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "eSxygoMr_Pox",
        "outputId": "816808b6-9be2-43b1-cce4-1f560ba6dbe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Training loss: 2.3823 training_acc: 0.2185 val_loss: 2.2437 val_acc: 0.2709\n",
            "#5 Training loss: 1.9053 training_acc: 0.3810 val_loss: 1.9093 val_acc: 0.3862\n",
            "#10 Training loss: 1.8277 training_acc: 0.4109 val_loss: 1.8682 val_acc: 0.4089\n",
            "#15 Training loss: 1.7919 training_acc: 0.4309 val_loss: 1.8063 val_acc: 0.4376\n",
            "#20 Training loss: 1.7583 training_acc: 0.4844 val_loss: 1.7800 val_acc: 0.4977\n",
            "#25 Training loss: 1.5869 training_acc: 0.5770 val_loss: 1.6078 val_acc: 0.5633\n",
            "#30 Training loss: 1.4984 training_acc: 0.6116 val_loss: 1.5442 val_acc: 0.5920\n",
            "#35 Training loss: 1.4650 training_acc: 0.6200 val_loss: 1.5256 val_acc: 0.6047\n",
            "#40 Training loss: 1.4503 training_acc: 0.6240 val_loss: 1.5202 val_acc: 0.5984\n",
            "#45 Training loss: 1.4413 training_acc: 0.6249 val_loss: 1.5157 val_acc: 0.5915\n",
            "#50 Training loss: 1.4950 training_acc: 0.6011 val_loss: 1.5358 val_acc: 0.5815\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([ 2,  9, 14,  ..., 10,  0,  4]),\n",
              " tensor([14,  9, 14,  ...,  9,  0,  4]))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_train(model, optimizer, criterion, trainloader, testloader, n_epochs=51, epoch_step=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVe2CM9u_Pox"
      },
      "outputs": [],
      "source": [
        "def predict_surname(surname):\n",
        "    surname = surname.lower()\n",
        "    return encoder.inverse_transform(model(trainset.vectorize(surname).view(1,-1)).argmax().view(-1))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85XDCXXd_Pox",
        "outputId": "0a6f4ab3-184f-44d2-d685-563ce7bff208"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'English'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_surname(\"Snow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2MIErKTo9aO"
      },
      "source": [
        "1.2 Замените модуль `RNN` из 1.1 на модули `nn.RNN`, `nn.LSTM` и `nn.GRU` (не забудьте указать аргумент `batch_first=True`). Сравните результаты работы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwHhiZA6_Poy"
      },
      "source": [
        "# nn.RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "560YapJK_Poy"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocab_len, pad_idx, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.rnncell = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.emb = nn.Embedding(\n",
        "            num_embeddings=vocab_len,\n",
        "            embedding_dim=input_size,\n",
        "            padding_idx=pad_idx\n",
        "        )\n",
        "        self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        output, h_n = self.rnncell(x)\n",
        "        x = self.fc(output[:,-1,:])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXJHFNpW_Poy"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    input_size=64,\n",
        "    hidden_size=16,\n",
        "    vocab_len=vocab.vocab_len,\n",
        "    pad_idx=0,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2ilGf3B_Poy",
        "outputId": "2ee93fbc-29a7-44a1-bbc0-599ac373eeb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Training loss: 2.2085 training_acc: 0.3560 val_loss: 2.0051 val_acc: 0.4144\n",
            "#5 Training loss: 1.7456 training_acc: 0.5326 val_loss: 1.7672 val_acc: 0.5287\n",
            "#10 Training loss: 1.6349 training_acc: 0.5552 val_loss: 1.6716 val_acc: 0.5455\n",
            "#15 Training loss: 1.5670 training_acc: 0.5795 val_loss: 1.6227 val_acc: 0.5679\n",
            "#20 Training loss: 1.5351 training_acc: 0.5907 val_loss: 1.5771 val_acc: 0.5760\n",
            "#25 Training loss: 1.4831 training_acc: 0.6105 val_loss: 1.5595 val_acc: 0.5934\n",
            "#30 Training loss: 1.4723 training_acc: 0.6149 val_loss: 1.5341 val_acc: 0.5965\n",
            "#35 Training loss: 1.4276 training_acc: 0.6284 val_loss: 1.5019 val_acc: 0.6097\n",
            "#40 Training loss: 1.4296 training_acc: 0.6273 val_loss: 1.5072 val_acc: 0.6029\n",
            "#45 Training loss: 1.4183 training_acc: 0.6298 val_loss: 1.4824 val_acc: 0.6107\n",
            "#50 Training loss: 1.3886 training_acc: 0.6354 val_loss: 1.4803 val_acc: 0.6043\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([ 2,  9, 14,  ..., 10,  0,  4]),\n",
              " tensor([ 9,  9, 14,  ..., 10,  0,  4]))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_train(model, optimizer, criterion, trainloader, testloader, n_epochs=51, epoch_step=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITEf0XIH_Poy",
        "outputId": "fa0eec23-7206-4353-dca7-672a89b80e29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'English'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_surname(\"Snow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sm8d48z_Poz"
      },
      "source": [
        "# nn.LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pbhPR-1_Poz"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocab_len, pad_idx, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.rnncell = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.emb = nn.Embedding(\n",
        "            num_embeddings=vocab_len,\n",
        "            embedding_dim=input_size,\n",
        "            padding_idx=pad_idx\n",
        "        )\n",
        "        self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        output, (h_n, c_n) = self.rnncell(x)\n",
        "        x = self.fc(output[:,-1,:])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IYRwQQo_Poz"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    input_size=64,\n",
        "    hidden_size=16,\n",
        "    vocab_len=vocab.vocab_len,\n",
        "    pad_idx=0,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODYwC3Zc_Poz",
        "outputId": "8f7c9a03-4664-4adb-f147-dd27ed06cdc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Training loss: 2.2987 training_acc: 0.2880 val_loss: 1.9933 val_acc: 0.4048\n",
            "#5 Training loss: 1.4157 training_acc: 0.6191 val_loss: 1.4490 val_acc: 0.6066\n",
            "#10 Training loss: 1.2682 training_acc: 0.6492 val_loss: 1.3316 val_acc: 0.6421\n",
            "#15 Training loss: 1.1781 training_acc: 0.6792 val_loss: 1.2981 val_acc: 0.6430\n",
            "#20 Training loss: 1.1234 training_acc: 0.6870 val_loss: 1.2352 val_acc: 0.6571\n",
            "#25 Training loss: 1.0821 training_acc: 0.6973 val_loss: 1.2120 val_acc: 0.6699\n",
            "#30 Training loss: 1.0328 training_acc: 0.7088 val_loss: 1.1901 val_acc: 0.6740\n",
            "#35 Training loss: 1.0009 training_acc: 0.7166 val_loss: 1.1659 val_acc: 0.6771\n",
            "#40 Training loss: 0.9811 training_acc: 0.7199 val_loss: 1.1499 val_acc: 0.6799\n",
            "#45 Training loss: 0.9419 training_acc: 0.7317 val_loss: 1.1386 val_acc: 0.6876\n",
            "#50 Training loss: 0.9234 training_acc: 0.7350 val_loss: 1.1220 val_acc: 0.6853\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([ 2,  9, 14,  ..., 10,  0,  4]),\n",
              " tensor([ 6,  9, 14,  ..., 10,  0,  4]))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_train(model, optimizer, criterion, trainloader, testloader, n_epochs=51, epoch_step=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAwUnZ9b_Poz",
        "outputId": "75a55b3a-11d9-4a19-f1e2-8fd216c2f336"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'English'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_surname(\"Snow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE41bH48_Po0"
      },
      "source": [
        "# nn.GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpBw00Zd_Po0"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocab_len, pad_idx, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.rnncell = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.emb = nn.Embedding(\n",
        "            num_embeddings=vocab_len,\n",
        "            embedding_dim=input_size,\n",
        "            padding_idx=pad_idx\n",
        "        )\n",
        "        self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        output, h_n = self.rnncell(x)\n",
        "        x = self.fc(output[:,-1,:])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NTFvsBb_Po0"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    input_size=64,\n",
        "    hidden_size=16,\n",
        "    vocab_len=vocab.vocab_len,\n",
        "    pad_idx=0,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL7Puq7d_Po0",
        "outputId": "d9b2ce2f-8e6b-46a1-aaae-f74f627865bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Training loss: 2.2792 training_acc: 0.2974 val_loss: 1.9990 val_acc: 0.3980\n",
            "#5 Training loss: 1.4117 training_acc: 0.6223 val_loss: 1.4257 val_acc: 0.6120\n",
            "#10 Training loss: 1.2042 training_acc: 0.6758 val_loss: 1.2715 val_acc: 0.6512\n",
            "#15 Training loss: 1.1010 training_acc: 0.7007 val_loss: 1.1925 val_acc: 0.6648\n",
            "#20 Training loss: 1.0387 training_acc: 0.7129 val_loss: 1.1567 val_acc: 0.6767\n",
            "#25 Training loss: 1.0063 training_acc: 0.7194 val_loss: 1.1241 val_acc: 0.6821\n",
            "#30 Training loss: 0.9768 training_acc: 0.7245 val_loss: 1.1037 val_acc: 0.6840\n",
            "#35 Training loss: 0.9659 training_acc: 0.7283 val_loss: 1.1017 val_acc: 0.6890\n",
            "#40 Training loss: 0.9306 training_acc: 0.7332 val_loss: 1.0889 val_acc: 0.6826\n",
            "#45 Training loss: 0.9163 training_acc: 0.7370 val_loss: 1.0883 val_acc: 0.6890\n",
            "#50 Training loss: 0.9094 training_acc: 0.7378 val_loss: 1.0943 val_acc: 0.6876\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([ 2,  9, 14,  ..., 10,  0,  4]),\n",
              " tensor([ 2,  9, 14,  ..., 10,  0,  4]))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_train(model, optimizer, criterion, trainloader, testloader, n_epochs=51, epoch_step=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq518rIf_Po0",
        "outputId": "ff582d41-29bf-4582-83d0-2858f3d16ab9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'English'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_surname(\"Snow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6YBam_3t-fO"
      },
      "source": [
        "1.3 Загрузите предобученные эмбеддинги (https://disk.yandex.ru/d/BHuT2tEXr_yBOQ?w=1) в модуль `nn.Embedding` и обучите модели из 1.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He0ksLn3_Po1"
      },
      "outputs": [],
      "source": [
        "with open('./data/glove.6B.50d.txt', encoding=\"utf8\") as file:\n",
        "    weights = file.readlines()\n",
        "weights = list(map(str.split, weights))\n",
        "weights = {i[0]: torch.tensor(list(map(float, i[1:]))) for i in weights}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fireQEXz_Po1",
        "outputId": "2d0dd678-0e21-430a-f486-1b327cd08990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "для ù нет эмбеддинга\n",
            "для ż нет эмбеддинга\n",
            "для ì нет эмбеддинга\n",
            "для ń нет эмбеддинга\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.2376,  0.9327,  0.6747,  ..., -0.4449,  0.2803,  1.0710],\n",
              "        [ 0.3818,  1.7446,  1.2457,  ...,  0.1289, -0.1232,  0.5867],\n",
              "        ...,\n",
              "        [-0.9432,  1.6448,  1.2020,  ..., -0.6479,  0.6564,  1.1344],\n",
              "        [ 0.2171,  0.4651, -0.4676,  ..., -0.0438,  0.4101,  0.1796],\n",
              "        [ 0.7383,  0.6545,  1.0873,  ..., -0.1680,  0.6562,  1.1014]],\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding = nn.Embedding(\n",
        "    num_embeddings=vocab.vocab_len,\n",
        "    embedding_dim=50,\n",
        "    padding_idx=0\n",
        ")\n",
        "for token in vocab.tokens:\n",
        "    try:\n",
        "        curr_weight = weights[token]\n",
        "        with torch.no_grad():\n",
        "            embedding.weight[vocab.token_to_idx[token]] = curr_weight\n",
        "    except KeyError as e:\n",
        "        print(f\"для {token} нет эмбеддинга\")\n",
        "embedding.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_wny0LJ_Po1"
      },
      "outputs": [],
      "source": [
        "minidict = {\n",
        "    \"ż\": \"z\",\n",
        "    \"ì\": \"i\",\n",
        "    \"ń\": \"n\",\n",
        "    \"ù\": \"u\"\n",
        "}\n",
        "def костыль(char_):\n",
        "    for key in minidict.keys():\n",
        "        char_ = char_.replace(key, str(minidict[key]))\n",
        "    return char_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbIjmn3b_Po1",
        "outputId": "3ba9e41f-5847-435a-b2cc-1d1e3bfadd4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.2376,  0.9327,  0.6747,  ..., -0.4449,  0.2803,  1.0710],\n",
              "        [ 0.3818,  1.7446,  1.2457,  ...,  0.1289, -0.1232,  0.5867],\n",
              "        ...,\n",
              "        [-0.9432,  1.6448,  1.2020,  ..., -0.6479,  0.6564,  1.1344],\n",
              "        [ 0.2171,  0.4651, -0.4676,  ..., -0.0438,  0.4101,  0.1796],\n",
              "        [ 0.7383,  0.6545,  1.0873,  ..., -0.1680,  0.6562,  1.1014]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding2 = nn.Embedding(\n",
        "    num_embeddings=vocab.vocab_len,\n",
        "    embedding_dim=50,\n",
        "    padding_idx=0\n",
        ")\n",
        "embedding2.weight.requires_grad = False\n",
        "for token in vocab.tokens:\n",
        "    token = костыль(token)\n",
        "    try:\n",
        "        curr_weight = weights[token]\n",
        "        embedding2.weight[vocab.token_to_idx[token]] = curr_weight\n",
        "    except KeyError as e:\n",
        "        print(f\"для {token} нет эмбеддинга\")\n",
        "embedding2.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE8tfwnN_Po2"
      },
      "source": [
        "# nn.RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDIzzQQh_Po2"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, emb, hidden_size, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.emb = emb\n",
        "        input_size = emb.weight.shape[1]\n",
        "        self.rnncell = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        output, h_n = self.rnncell(x)\n",
        "        x = self.fc(output[:,-1,:])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS1hstVv_Po3"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    emb=deepcopy(embedding),\n",
        "    hidden_size=16,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jErcLBhs_Po3",
        "outputId": "14e021f2-27f2-4180-f8fb-38b52ce334b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Training loss: 2.3355 training_acc: 0.2556 val_loss: 2.2431 val_acc: 0.2709\n",
            "#5 Training loss: 1.8937 training_acc: 0.3786 val_loss: 1.9002 val_acc: 0.3953\n",
            "#10 Training loss: 1.8273 training_acc: 0.4010 val_loss: 1.8776 val_acc: 0.4039\n",
            "#15 Training loss: 1.7800 training_acc: 0.4285 val_loss: 1.8229 val_acc: 0.4267\n",
            "#20 Training loss: 1.7563 training_acc: 0.4402 val_loss: 1.8027 val_acc: 0.4349\n",
            "#25 Training loss: 1.7133 training_acc: 0.4553 val_loss: 1.7803 val_acc: 0.4349\n",
            "#30 Training loss: 1.5535 training_acc: 0.5850 val_loss: 1.5948 val_acc: 0.5692\n",
            "#35 Training loss: 1.5075 training_acc: 0.6028 val_loss: 1.5275 val_acc: 0.5943\n",
            "#40 Training loss: 1.4649 training_acc: 0.6119 val_loss: 1.5274 val_acc: 0.5911\n",
            "#45 Training loss: 1.4244 training_acc: 0.6252 val_loss: 1.4815 val_acc: 0.6011\n",
            "#50 Training loss: 1.4472 training_acc: 0.6149 val_loss: 1.4783 val_acc: 0.6029\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([ 2,  9, 14,  ..., 10,  0,  4]),\n",
              " tensor([14,  9, 14,  ..., 10, 10, 14]))"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_train(model, optimizer, criterion, trainloader, testloader, n_epochs=51, epoch_step=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHQia6wA_Po3",
        "outputId": "5e563f0c-784d-440f-96fb-859b251c5f01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'English'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_surname(\"Snow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5KIYm2c_Po4"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    emb=deepcopy(embedding2),\n",
        "    hidden_size=16,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uCUuC3m_Po4",
        "outputId": "20111471-7413-4943-87e0-5ff165512b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Training loss: 2.3161 training_acc: 0.2441 val_loss: 2.2431 val_acc: 0.2709\n",
            "#5 Training loss: 2.2082 training_acc: 0.2714 val_loss: 2.2038 val_acc: 0.2723\n",
            "#10 Training loss: 1.7931 training_acc: 0.4567 val_loss: 1.7935 val_acc: 0.4745\n",
            "#15 Training loss: 1.6127 training_acc: 0.5512 val_loss: 1.6536 val_acc: 0.5369\n",
            "#20 Training loss: 1.5537 training_acc: 0.5647 val_loss: 1.5818 val_acc: 0.5578\n",
            "#25 Training loss: 1.5050 training_acc: 0.5860 val_loss: 1.5437 val_acc: 0.5692\n",
            "#30 Training loss: 1.4834 training_acc: 0.5937 val_loss: 1.5191 val_acc: 0.5801\n",
            "#35 Training loss: 1.4689 training_acc: 0.5965 val_loss: 1.5031 val_acc: 0.5797\n",
            "#40 Training loss: 1.4417 training_acc: 0.6063 val_loss: 1.4756 val_acc: 0.5865\n",
            "#45 Training loss: 1.4298 training_acc: 0.6085 val_loss: 1.4618 val_acc: 0.5915\n",
            "#50 Training loss: 1.4240 training_acc: 0.6117 val_loss: 1.4919 val_acc: 0.5902\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([ 2,  9, 14,  ..., 10,  0,  4]),\n",
              " tensor([ 4, 10, 14,  ..., 10,  0,  4]))"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_train(model, optimizer, criterion, trainloader, testloader, n_epochs=51, epoch_step=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQHCidPw_Po5"
      },
      "source": [
        "# nn.LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-ukNCUm_Po5"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, emb, hidden_size, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.emb = emb\n",
        "        input_size = emb.weight.shape[1]\n",
        "        self.rnncell = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        output, (h_n, c_n) = self.rnncell(x)\n",
        "        x = self.fc(output[:,-1,:])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWiFGOeV_Po5"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    emb=embedding2,\n",
        "    hidden_size=16,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2n5X1Cg_Po5",
        "outputId": "b7d802dd-7c98-41ab-abdd-d3db36e00511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Training loss: 2.3721 training_acc: 0.2375 val_loss: 2.2310 val_acc: 0.2709\n",
            "#5 Training loss: 1.5366 training_acc: 0.5718 val_loss: 1.5287 val_acc: 0.5783\n",
            "#10 Training loss: 1.3457 training_acc: 0.6293 val_loss: 1.3482 val_acc: 0.6234\n",
            "#15 Training loss: 1.2410 training_acc: 0.6695 val_loss: 1.2766 val_acc: 0.6462\n",
            "#20 Training loss: 1.1749 training_acc: 0.6869 val_loss: 1.2141 val_acc: 0.6676\n",
            "#25 Training loss: 1.1281 training_acc: 0.6935 val_loss: 1.1734 val_acc: 0.6749\n",
            "#30 Training loss: 1.0839 training_acc: 0.6987 val_loss: 1.1353 val_acc: 0.6799\n",
            "#35 Training loss: 1.0488 training_acc: 0.7078 val_loss: 1.1069 val_acc: 0.6885\n",
            "#40 Training loss: 1.0214 training_acc: 0.7127 val_loss: 1.0904 val_acc: 0.6926\n",
            "#45 Training loss: 0.9904 training_acc: 0.7226 val_loss: 1.0730 val_acc: 0.6935\n",
            "#50 Training loss: 0.9704 training_acc: 0.7286 val_loss: 1.0807 val_acc: 0.6944\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([ 2,  9, 14,  ..., 10,  0,  4]),\n",
              " tensor([14,  9, 14,  ..., 10,  0,  4]))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_train(model, optimizer, criterion, trainloader, testloader, n_epochs=51, epoch_step=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZtMkxI3_Po6",
        "outputId": "7e6c5285-7062-44bc-cd9b-9056ae73594c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'English'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_surname(\"Snow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afg_xkuc_Po6"
      },
      "source": [
        "# nn.GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EnvFGSI_Po6"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, emb, hidden_size, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.emb = emb\n",
        "        input_size = emb.weight.shape[1]\n",
        "        self.rnncell = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        output, h_n = self.rnncell(x)\n",
        "        x = self.fc(output[:,-1,:])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5DrHNEn_Po6"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    emb=embedding2,\n",
        "    hidden_size=16,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSNmgyGR_Po6",
        "outputId": "c203b233-c3f4-462a-d842-dbe2262fa726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Training loss: 2.3386 training_acc: 0.2575 val_loss: 2.1376 val_acc: 0.2723\n",
            "#5 Training loss: 1.3976 training_acc: 0.5986 val_loss: 1.3987 val_acc: 0.5934\n",
            "#10 Training loss: 1.2280 training_acc: 0.6556 val_loss: 1.2775 val_acc: 0.6343\n",
            "#15 Training loss: 1.1440 training_acc: 0.6802 val_loss: 1.2063 val_acc: 0.6571\n",
            "#20 Training loss: 1.0864 training_acc: 0.7028 val_loss: 1.1502 val_acc: 0.6817\n",
            "#25 Training loss: 1.0392 training_acc: 0.7128 val_loss: 1.1233 val_acc: 0.6899\n",
            "#30 Training loss: 1.0025 training_acc: 0.7203 val_loss: 1.1043 val_acc: 0.6899\n",
            "#35 Training loss: 0.9713 training_acc: 0.7273 val_loss: 1.0823 val_acc: 0.6981\n",
            "#40 Training loss: 0.9502 training_acc: 0.7344 val_loss: 1.0659 val_acc: 0.7017\n",
            "#45 Training loss: 0.9266 training_acc: 0.7407 val_loss: 1.0598 val_acc: 0.7022\n",
            "#50 Training loss: 0.9106 training_acc: 0.7442 val_loss: 1.0394 val_acc: 0.7058\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([ 2,  9, 14,  ..., 10,  0,  4]),\n",
              " tensor([ 2,  9, 14,  ..., 10,  0,  4]))"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_train(model, optimizer, criterion, trainloader, testloader, n_epochs=51, epoch_step=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FCeyh9n_Po7",
        "outputId": "0bcb1b06-9ff1-4544-bf37-99b89280a672"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'English'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_surname(\"Snow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7kf990U9Do-"
      },
      "source": [
        "## 2. Классификация обзоров на фильмы (RNN)\n",
        "\n",
        "Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg\n",
        "\n",
        "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
        "  * токен = __слово__\n",
        "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
        "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
        "  * добавьте предобработку текста\n",
        "\n",
        "2.2. Обучите классификатор.\n",
        "  \n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding` \n",
        "    - подберите адекватную размерность вектора эмбеддинга: \n",
        "    - модуль `nn.Embedding` обучается\n",
        "\n",
        "  * Используйте рекуррентные слои (`nn.RNN`, `nn.LSTM`, `nn.GRU`)\n",
        "\n",
        "\n",
        "2.3 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
        "* Целевое значение accuracy на валидации - 70+%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZWC94gs_Po7",
        "outputId": "d1fbe274-ca50-4168-da54-aa68941f4259"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>simplistic , silly and tedious .</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it's so laddish and juvenile , only teenage bo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review      type\n",
              "0                   simplistic , silly and tedious .  positive\n",
              "1  it's so laddish and juvenile , only teenage bo...  positive"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "positive = pd.read_csv(\"./data/positive_reviews.txt\", sep='%-%', header=None, engine=\"python\")\n",
        "positive[\"type\"] = \"positive\"\n",
        "negative = pd.read_csv(\"./data/negative_reviews.txt\", sep='%-%', header=None, engine=\"python\")\n",
        "negative[\"type\"] = \"negative\"\n",
        "df = pd.concat((positive, negative), ignore_index=True)\n",
        "df.columns = [\"review\", \"type\"]\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj9igN7a_Po7"
      },
      "outputs": [],
      "source": [
        "encoder2 = LabelEncoder()\n",
        "y = encoder2.fit_transform(df.type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45Fwr7EK_Po8"
      },
      "outputs": [],
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "puttern = re.compile(\"^[a-z]+$\")\n",
        "def preprocess(text):\n",
        "    text = text.lower().strip()\n",
        "    words = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text) if (puttern.search(word)) and (word not in stopwords)]\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_MGjbXR_Po8"
      },
      "outputs": [],
      "source": [
        "X = df.review.apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pxuCJhl_Po8"
      },
      "outputs": [],
      "source": [
        "n_classes = np.unique(y).size\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaWIxfie_Po8"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, data):\n",
        "        self.max_seq_len = data.apply(lambda x: len(x)).max()\n",
        "        tokens = set()\n",
        "        for item in data:\n",
        "            tokens.update(item)\n",
        "        self.idx_to_token = dict(enumerate(tokens, 2))\n",
        "        self.idx_to_token[0] = \"<PAD>\"\n",
        "        self.idx_to_token[1] = \"<UNK>\"\n",
        "        self.token_to_idx = {token: idx for idx, token in self.idx_to_token.items()}\n",
        "        self.vocab_len = len(self.idx_to_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srf-srkp_Po8"
      },
      "outputs": [],
      "source": [
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, X, y, vocab: Vocab):\n",
        "        self.X = X\n",
        "        self.y = torch.LongTensor(y)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def vectorize(self, review):\n",
        "        '''Генерирует представление отзыва'''\n",
        "        m_len = self.vocab.max_seq_len\n",
        "        review = review[:m_len]\n",
        "#         review_t = torch.zeros(self.vocab.max_seq_len).type(torch.long)\n",
        "        review_t = torch.zeros(m_len).type(torch.long)\n",
        "        shift = m_len - len(review)\n",
        "        for i, token in enumerate(review):\n",
        "            try:\n",
        "                review_t[shift+i] = self.vocab.token_to_idx[token]\n",
        "            except KeyError as ke:\n",
        "                review_t[shift+i] = self.vocab.token_to_idx[\"<UNK>\"]\n",
        "#         review_t = torch.cat((torch.zeros(m_len-review_t.size(0)), review_t))\n",
        "#         review_t\n",
        "        return review_t\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.vectorize(self.X.iloc[idx]), self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bORxrfBK_Po9"
      },
      "outputs": [],
      "source": [
        "vocab = Vocab(X_train)\n",
        "train_dataset = ReviewsDataset(X_train, y_train, vocab)\n",
        "test_dataset = ReviewsDataset(X_test, y_test, vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lof9Ui2t_Po9"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, vocab_len, emb_dim, hidden_size, num_classes, n_layers=1):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_len, emb_dim, padding_idx=0)\n",
        "        self.rnncell = nn.RNN(emb_dim, hidden_size, num_layers=n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "  \n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x, h_n = self.rnncell(x)\n",
        "#         print(h_n.shape, x.shape)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "#         x = self.fc(h_n[0].squeeze(0))\n",
        "#         print(x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRFZUEgp_Po9"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab.vocab_len,\n",
        "    emb_dim=64,\n",
        "    hidden_size=16,\n",
        "    num_classes=2,\n",
        "    n_layers=2,\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXfvYoxy_Po9",
        "outputId": "dea59d87-370d-4cba-a74b-fd52a653ede5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Training loss: 0.6971 training_acc: 0.5081 val_loss: 0.6918 val_acc: 0.5246\n",
            "#5 Training loss: 0.6891 training_acc: 0.5416 val_loss: 0.6917 val_acc: 0.5180\n",
            "#10 Training loss: 0.6881 training_acc: 0.5460 val_loss: 0.6916 val_acc: 0.5157\n",
            "#15 Training loss: 0.6857 training_acc: 0.5488 val_loss: 0.6913 val_acc: 0.5204\n",
            "#20 Training loss: 0.6781 training_acc: 0.5683 val_loss: 0.6916 val_acc: 0.5218\n",
            "#25 Training loss: 0.6704 training_acc: 0.5881 val_loss: 0.6925 val_acc: 0.5340\n",
            "#30 Training loss: 0.6517 training_acc: 0.6199 val_loss: 0.7125 val_acc: 0.5518\n",
            "#35 Training loss: 0.6052 training_acc: 0.6730 val_loss: 0.6787 val_acc: 0.5968\n",
            "#40 Training loss: 0.5181 training_acc: 0.7477 val_loss: 0.6441 val_acc: 0.6512\n",
            "#45 Training loss: 0.3994 training_acc: 0.8220 val_loss: 0.6274 val_acc: 0.6906\n",
            "#50 Training loss: 0.2704 training_acc: 0.8931 val_loss: 0.6980 val_acc: 0.6910\n"
          ]
        }
      ],
      "source": [
        "y_true, y_pred = model_train(model, optimizer, criterion, train_loader, test_loader, n_epochs=101, epoch_step=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFthCGzw_Po9",
        "outputId": "ce072b96-fb8b-408e-f063-c34b0566e921"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[843, 224],\n",
              "       [435, 631]], dtype=int64)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confusion_matrix(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIrXRc3v_Po9"
      },
      "outputs": [],
      "source": [
        "def test_review(review):\n",
        "    x = train_dataset.vectorize(review).unsqueeze(0)\n",
        "    variety, predictions = model(x).softmax(1).topk(k=1, dim=1)\n",
        "    variety = variety.cpu().detach().view(-1)\n",
        "    pred_ = predictions.cpu().detach().view(-1)\n",
        "    pred_ = encoder2.inverse_transform(pred_)\n",
        "    out_ = \", \".join([f\"{nat}:{frac:.2f}\" for nat, frac in zip(pred_, variety)])\n",
        "    print(f\"{review} --- {out_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NfhLL8U_Po-"
      },
      "outputs": [],
      "source": [
        "reviews_ = [\n",
        "    \"The last time I had lunch here, I really liked the soup. Excellent. I'll come again.\",\n",
        "    \"It feels like I dined on frogs. I will not visit this institution again.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbbo0bjC_Po-",
        "outputId": "51f7c010-113a-4d50-bb2e-72a8154eca57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['last', 'time', 'lunch', 'really', 'liked', 'soup', 'excellent', 'come'] --- positive:0.98\n",
            "['feel', 'like', 'dined', 'frog', 'visit', 'institution'] --- positive:0.60\n"
          ]
        }
      ],
      "source": [
        "for review in reviews_:\n",
        "     test_review(preprocess(review))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}