{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtFQP3RNll3c",
    "outputId": "ef65051e-9af7-4f41-f52c-b769b9b1a036"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx75RigN8xIJ"
   },
   "source": [
    "## 1. Представление и предобработка текстовых данных в виде последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LScKIAey9dAM"
   },
   "source": [
    "1.1 Представьте первое предложение из строки `text` как последовательность из индексов слов, входящих в это предложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "phEw721T9SYW"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'supported': 0,\n",
       " 'c++': 1,\n",
       " 'stable': 2,\n",
       " 'most': 3,\n",
       " 'note': 4,\n",
       " 'of': 5,\n",
       " 'select': 6,\n",
       " 'currently': 7,\n",
       " 'pytorch': 8,\n",
       " 'version': 9,\n",
       " 'your': 10,\n",
       " 'only': 11,\n",
       " 'and': 12,\n",
       " 'that': 13,\n",
       " 'run': 14,\n",
       " 'command': 15,\n",
       " 'is': 16,\n",
       " 'represents': 17,\n",
       " 'tested': 18,\n",
       " 'the': 19,\n",
       " 'install': 20,\n",
       " 'for': 21,\n",
       " 'libtorch': 22,\n",
       " 'preferences': 23,\n",
       " 'available': 24}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = re.compile(\"[A-Za-z]+\")\n",
    "text_words = set(word for word in nltk.word_tokenize(text.lower()) if sub.search(word))\n",
    "ind_to_word = dict(enumerate(text_words))\n",
    "word_to_ind = {j:i for i,j in ind_to_word.items()}\n",
    "word_to_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nje3DVCNmyln",
    "outputId": "9200f4f3-71db-4da1-aa02-5f186484aebe"
   },
   "outputs": [],
   "source": [
    "fs = 'Select your preferences and run the install command'\n",
    "fs = fs.lower()\n",
    "fs = nltk.word_tokenize(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EY_qM8wZoCFm",
    "outputId": "852a9d53-ec9a-4e99-bcf9-fc04d99f7c0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 10., 23., 12., 14., 19., 20., 15.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_vect = torch.zeros(len(fs))\n",
    "for i,word in enumerate(fs):\n",
    "    fs_vect[i] = word_to_ind[word]\n",
    "fs_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSFQCPtD9x5J"
   },
   "source": [
    "1.2 Представьте первое предложение из строки `text` как последовательность векторов, соответствующих индексам слов. Для представления индекса в виде вектора используйте унитарное кодирование. В результате должен получиться двумерный тензор размера `количество слов в предложении` x `количество уникальных слов`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZS4XLV0-buf"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_matrx = torch.zeros((len(fs), len(word_to_ind)))\n",
    "for i, word in enumerate(fs):\n",
    "    fs_matrx[i, word_to_ind[word]] = 1\n",
    "fs_matrx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZvQKHYA-mJN"
   },
   "source": [
    "1.3 Решите задачу 1.2, используя модуль `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 10, 23, 12, 14, 19, 20, 15])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_vect_ = fs_vect.type(torch.long)\n",
    "fs_vect_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5549, -1.3717,  0.8663,  0.3970,  1.7596,  0.1559,  1.6309,  0.9894,\n",
       "          0.3076, -0.7276, -0.1996, -0.4460,  0.5722,  0.8552, -0.7268,  1.0791,\n",
       "         -0.1829, -0.6409,  1.8228, -2.0906, -0.7622, -1.3581, -1.0597, -0.3444,\n",
       "          0.7982],\n",
       "        [-0.1200,  2.0094, -1.3228,  0.0124,  1.7570,  0.5700, -0.2939, -1.2750,\n",
       "          0.9414, -0.8880,  0.4092, -0.8743, -0.4598,  0.0810,  1.1493,  1.2512,\n",
       "         -2.5529,  1.4449, -1.3563,  1.8004, -0.8388,  1.0175,  0.9995, -0.8984,\n",
       "          1.7567],\n",
       "        [ 0.1817,  0.2718,  0.1625,  0.2713,  0.5173, -0.7460, -1.3254,  1.2003,\n",
       "          1.2920, -1.0720, -0.1621,  2.1803, -1.6398, -0.7761, -0.0253, -0.3782,\n",
       "         -0.7283,  1.2035, -0.5046, -0.8360,  0.0859, -2.2821,  0.7486,  0.2217,\n",
       "         -1.1833],\n",
       "        [ 0.6319, -1.6133,  1.3906,  0.3368,  0.0168, -1.3093,  1.1511,  0.6371,\n",
       "          0.1609, -0.0249, -0.2789, -0.5414,  1.0936, -0.4611,  0.7372,  0.3766,\n",
       "         -0.4738, -1.1117, -0.7908,  1.1568, -0.7415,  1.0956,  0.0248,  0.4099,\n",
       "          0.3755],\n",
       "        [ 0.0801,  0.6744, -2.9442, -0.3443, -0.5703,  0.1367,  0.9246, -0.2802,\n",
       "          0.4629, -0.1363,  0.4149,  0.9643,  0.7604, -0.1572,  0.8815,  2.3867,\n",
       "         -1.4501, -0.4275,  0.4373,  0.1807,  1.4505,  0.5258,  1.4122,  1.1183,\n",
       "         -0.0099],\n",
       "        [ 0.7631, -0.1228,  1.0654,  0.7281, -1.3044,  1.3042, -1.0253, -1.6398,\n",
       "          0.2402, -0.1726, -1.1703, -1.2713, -0.2402, -0.6825,  0.4651,  0.5238,\n",
       "         -0.0461,  1.0587,  0.3974, -0.5151,  0.4433, -1.3931, -0.7701,  1.4849,\n",
       "         -1.2777],\n",
       "        [-0.4640,  2.0634, -1.0483, -0.1096,  0.6981, -0.7409, -0.5191, -1.5285,\n",
       "         -0.1678, -0.2387, -0.3268, -0.0587,  0.2076, -0.2632,  0.6298,  0.0226,\n",
       "          1.3492, -0.5965,  1.2585, -0.0656, -0.8901, -0.9760, -0.5645, -1.3110,\n",
       "          0.9539],\n",
       "        [-0.3586, -1.7192,  0.4874, -1.0596,  1.1387, -0.6004,  0.0177, -0.6571,\n",
       "          1.1189, -1.3212, -0.4912, -0.5292, -0.7845, -0.2695,  0.4191, -1.2616,\n",
       "          0.3637, -0.6578,  1.3114, -1.1686,  1.8242, -0.6655,  1.8373,  0.6341,\n",
       "         -0.2996]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Embedding(num_embeddings=len(text_words), embedding_dim=len(text_words))(fs_vect_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXjM7qEUNFY_"
   },
   "source": [
    "## 2. Классификация фамилий по национальности (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
    "\n",
    "2.1 Считать файл `surnames/surnames.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/surnames.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Закодировать национальности числами, начиная с 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_encoder = LabelEncoder()\n",
    "df['nationality'] = class_encoder.fit_transform(df['nationality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Разбить датасет на обучающую и тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['surname'].str.lower().str.strip()\n",
    "y = df['nationality']\n",
    "n_classes = y.nunique()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
    "  * добавьте в словарь специальный токен `<PAD>` с индексом 0\n",
    "  * при создании словаря сохраните длину самой длинной последовательности из набора данных в виде атрибута `max_seq_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, data):\n",
    "        self.max_seq_len = data.str.len().max()\n",
    "        tokens = set()\n",
    "        for item in data:\n",
    "            tokens.update(item)\n",
    "        tokens = list(tokens)\n",
    "        tokens.insert(0, \"<PAD>\")\n",
    "        self.idx_to_token = dict(enumerate(tokens))\n",
    "        self.token_to_idx = {token: idx for idx, token in self.idx_to_token.items()}\n",
    "        self.vocab_len = len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "  * метод `__getitem__` возвращает пару: <последовательность индексов токенов (см. 1.1 ), номер класса> \n",
    "  * длина каждой такой последовательности должна быть одинаковой и равной `vocab.max_seq_len`. Чтобы добиться этого, дополните последовательность справа индексом токена `<PAD>` до нужной длины\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "    def __init__(self, X, y, vocab: Vocab):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        '''Генерирует представление фамилии surname в при помощи бинарного кодирования (см. 1.2)'''\n",
    "        surname_t = torch.zeros(self.vocab.max_seq_len).type(torch.long)\n",
    "        for i, token in enumerate(surname):\n",
    "            surname_t[i] = self.vocab.token_to_idx[token]\n",
    "        return surname_t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.vectorize(self.X.iloc[idx]), self.y.iloc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6. Обучить классификатор.\n",
    "  \n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`. Рассмотрите два варианта: \n",
    "    - когда токен представляется в виде унитарного вектора и модуль `nn.Embedding` не обучается\n",
    "    - когда токен представляется в виде вектора небольшой размерности (меньше, чем размер словаря) и модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_training(model, criterion, optimizer, n_epochs=51):\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        y_pred = torch.empty(0)\n",
    "        y_true = torch.empty(0)\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch.type(torch.long))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_loss += loss.item()\n",
    "            with torch.no_grad():\n",
    "                y_true = torch.cat((y_true, y_batch))\n",
    "                y_pred = torch.cat((y_pred, predictions.argmax(dim=1).cpu().detach()))\n",
    "        trin_acc = accuracy_score(y_true, y_pred).item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            y_pred = torch.empty(0)\n",
    "            y_true = torch.empty(0)\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions, y_batch.type(torch.long)).item()\n",
    "                y_true = torch.cat((y_true, y_batch))\n",
    "                y_pred = torch.cat((y_pred, predictions.argmax(dim=1).cpu().detach()))\n",
    "                val_loss += loss\n",
    "            val_acc = accuracy_score(y_true, y_pred).item()\n",
    "            if epoch % 5 == 0:\n",
    "                print(f'#{epoch} Training loss: {epoch_loss / len(train_loader):.4f}\\\n",
    " training_acc: {trin_acc:.4f} val_loss: {val_loss / len(test_loader):.4f} val_acc: {val_acc:.4f}')\n",
    "                \n",
    "def GloP_boB(model):\n",
    "    y_pred = torch.empty(0)\n",
    "    y_ = torch.empty(0)\n",
    "\n",
    "    for X_batch, y_batch in test_loader:\n",
    "\n",
    "        predictions = model(X_batch).argmax(dim=1).cpu().detach()\n",
    "        y_pred = torch.cat((y_pred, predictions))\n",
    "        y_ = torch.cat((y_, y_batch))\n",
    "\n",
    "    print(classification_report(y_, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(X)\n",
    "train_dataset = SurnamesDataset(X_train, y_train, vocab)\n",
    "test_dataset = SurnamesDataset(X_test, y_test, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOwnClassifier(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(MyOwnClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv1d(embedding_dim, 64, kernel_size=5),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(2),\n",
    "        )\n",
    "#         self.l2 = nn.Sequential(\n",
    "#             nn.Conv1d(64, 128, kernel_size=3),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.MaxPool1d(2),\n",
    "#         )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(3*128, 1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 18),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        x = self.embedding(x).transpose(1,2)\n",
    "        x = self.l1(x)\n",
    "#         x = self.l2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyOwnClassifier(num_embeddings=vocab.vocab_len, embedding_dim=1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Training loss: 2.6574 training_acc: 0.3266 val_loss: 2.6476 val_acc: 0.3347\n",
      "#5 Training loss: 2.6069 training_acc: 0.3749 val_loss: 2.6026 val_acc: 0.3780\n",
      "#10 Training loss: 2.5764 training_acc: 0.4041 val_loss: 2.5774 val_acc: 0.4053\n",
      "#15 Training loss: 2.5674 training_acc: 0.4128 val_loss: 2.5733 val_acc: 0.4080\n",
      "#20 Training loss: 2.5451 training_acc: 0.4352 val_loss: 2.5413 val_acc: 0.4399\n",
      "#25 Training loss: 2.5310 training_acc: 0.4512 val_loss: 2.5237 val_acc: 0.4577\n",
      "#30 Training loss: 2.5213 training_acc: 0.4604 val_loss: 2.5195 val_acc: 0.4577\n",
      "#35 Training loss: 2.5085 training_acc: 0.4735 val_loss: 2.5087 val_acc: 0.4672\n",
      "#40 Training loss: 2.5026 training_acc: 0.4788 val_loss: 2.4957 val_acc: 0.4827\n",
      "#45 Training loss: 2.4913 training_acc: 0.4909 val_loss: 2.5044 val_acc: 0.4740\n",
      "#50 Training loss: 2.4874 training_acc: 0.4950 val_loss: 2.4926 val_acc: 0.4882\n"
     ]
    }
   ],
   "source": [
    "hard_training(model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.79      0.62       346\n",
      "         1.0       0.00      0.00      0.00        36\n",
      "         2.0       0.00      0.00      0.00        81\n",
      "         3.0       0.00      0.00      0.00        49\n",
      "         4.0       0.41      0.80      0.54       567\n",
      "         5.0       0.00      0.00      0.00        36\n",
      "         6.0       0.00      0.00      0.00       118\n",
      "         7.0       0.00      0.00      0.00        32\n",
      "         8.0       0.00      0.00      0.00        41\n",
      "         9.0       0.00      0.00      0.00       108\n",
      "        10.0       0.47      0.35      0.40       161\n",
      "        11.0       0.00      0.00      0.00        15\n",
      "        12.0       0.00      0.00      0.00        25\n",
      "        13.0       0.00      0.00      0.00        14\n",
      "        14.0       0.64      0.58      0.61       482\n",
      "        15.0       0.00      0.00      0.00        13\n",
      "        16.0       0.00      0.00      0.00        57\n",
      "        17.0       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.48      2196\n",
      "   macro avg       0.11      0.14      0.12      2196\n",
      "weighted avg       0.36      0.48      0.40      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GloP_boB(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MyOwnClassifier(num_embeddings=vocab.vocab_len, embedding_dim=32)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Training loss: 2.5646 training_acc: 0.4235 val_loss: 2.4824 val_acc: 0.4954\n",
      "#5 Training loss: 2.4261 training_acc: 0.5556 val_loss: 2.4259 val_acc: 0.5524\n",
      "#10 Training loss: 2.4004 training_acc: 0.5814 val_loss: 2.4085 val_acc: 0.5669\n",
      "#15 Training loss: 2.3654 training_acc: 0.6169 val_loss: 2.3710 val_acc: 0.6088\n",
      "#20 Training loss: 2.3509 training_acc: 0.6302 val_loss: 2.3639 val_acc: 0.6189\n",
      "#25 Training loss: 2.3232 training_acc: 0.6601 val_loss: 2.3448 val_acc: 0.6362\n",
      "#30 Training loss: 2.3097 training_acc: 0.6717 val_loss: 2.3207 val_acc: 0.6585\n",
      "#35 Training loss: 2.3022 training_acc: 0.6793 val_loss: 2.3244 val_acc: 0.6507\n",
      "#40 Training loss: 2.2963 training_acc: 0.6851 val_loss: 2.3195 val_acc: 0.6594\n",
      "#45 Training loss: 2.2878 training_acc: 0.6943 val_loss: 2.3330 val_acc: 0.6448\n",
      "#50 Training loss: 2.2849 training_acc: 0.6967 val_loss: 2.3102 val_acc: 0.6648\n",
      "#55 Training loss: 2.2819 training_acc: 0.6997 val_loss: 2.3161 val_acc: 0.6644\n",
      "#60 Training loss: 2.2786 training_acc: 0.7028 val_loss: 2.3113 val_acc: 0.6644\n",
      "#65 Training loss: 2.2783 training_acc: 0.7039 val_loss: 2.3177 val_acc: 0.6617\n",
      "#70 Training loss: 2.2782 training_acc: 0.7036 val_loss: 2.3162 val_acc: 0.6630\n",
      "#75 Training loss: 2.2721 training_acc: 0.7096 val_loss: 2.3153 val_acc: 0.6644\n",
      "#80 Training loss: 2.2749 training_acc: 0.7057 val_loss: 2.3135 val_acc: 0.6658\n",
      "#85 Training loss: 2.2703 training_acc: 0.7116 val_loss: 2.3113 val_acc: 0.6648\n",
      "#90 Training loss: 2.2663 training_acc: 0.7152 val_loss: 2.3006 val_acc: 0.6762\n",
      "#95 Training loss: 2.2637 training_acc: 0.7174 val_loss: 2.3020 val_acc: 0.6749\n",
      "#100 Training loss: 2.2612 training_acc: 0.7207 val_loss: 2.2985 val_acc: 0.6812\n"
     ]
    }
   ],
   "source": [
    "hard_training(model2, criterion, optimizer, n_epochs=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.91      0.90       346\n",
      "         1.0       0.48      0.86      0.61        36\n",
      "         2.0       0.00      0.00      0.00        81\n",
      "         3.0       0.00      0.00      0.00        49\n",
      "         4.0       0.58      0.92      0.71       567\n",
      "         5.0       0.00      0.00      0.00        36\n",
      "         6.0       0.44      0.16      0.24       118\n",
      "         7.0       0.00      0.00      0.00        32\n",
      "         8.0       0.00      0.00      0.00        41\n",
      "         9.0       0.49      0.74      0.59       108\n",
      "        10.0       0.77      0.85      0.81       161\n",
      "        11.0       0.00      0.00      0.00        15\n",
      "        12.0       0.00      0.00      0.00        25\n",
      "        13.0       0.00      0.00      0.00        14\n",
      "        14.0       0.80      0.79      0.80       482\n",
      "        15.0       0.00      0.00      0.00        13\n",
      "        16.0       0.00      0.00      0.00        57\n",
      "        17.0       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.68      2196\n",
      "   macro avg       0.25      0.29      0.26      2196\n",
      "weighted avg       0.58      0.68      0.61      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GloP_boB(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "id": "ZGfJX2NP1sw4"
   },
   "outputs": [],
   "source": [
    "def test_surname(surname):\n",
    "    x = train_dataset.vectorize(surname).unsqueeze(0)\n",
    "    variety, predictions = model(x).topk(k=3, dim=1)\n",
    "    variety = variety.cpu().detach().squeeze()\n",
    "    pred_ = class_encoder.inverse_transform(predictions.cpu().detach().squeeze())\n",
    "    out_ = \", \".join([f\"{nat}:{frac:.2f}\" for nat, frac in zip(pred_, variety)])\n",
    "    print(f\"{surname} --- {out_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "GHjCRqQg1sw5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bazoyan --- Russian:1.00, Japanese:0.00, English:0.00\n",
      "gorshenev --- Russian:0.99, English:0.01, Greek:0.00\n",
      "strizhov --- Russian:1.00, English:0.00, Greek:0.00\n",
      "petukhov --- Russian:1.00, English:0.00, Greek:0.00\n",
      "trubadurov --- Russian:1.00, Japanese:0.00, German:0.00\n",
      "atayanc --- English:1.00, Arabic:0.00, Greek:0.00\n"
     ]
    }
   ],
   "source": [
    "for surname in (\"bazoyan\", \"gorshenev\", \"strizhov\", \"petukhov\", \"trubadurov\", \"atayanc\"):\n",
    "    test_surname(surname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo-hf5CQ0iWv"
   },
   "source": [
    "## 3. Классификация обзоров на фильмы (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg\n",
    "\n",
    "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
    "  * токен = __слово__\n",
    "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
    "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
    "  * добавьте предобработку текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simplistic , silly and tedious .</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it's so laddish and juvenile , only teenage bo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review      type\n",
       "0                   simplistic , silly and tedious .  positive\n",
       "1  it's so laddish and juvenile , only teenage bo...  positive"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = pd.read_csv(\"./data/positive_reviews.txt\", sep='%-%', header=None, engine=\"python\")\n",
    "positive[\"type\"] = \"positive\"\n",
    "negative = pd.read_csv(\"./data/negative_reviews.txt\", sep='%-%', header=None, engine=\"python\")\n",
    "negative[\"type\"] = \"negative\"\n",
    "df = pd.concat((positive, negative), ignore_index=True)\n",
    "df.columns = [\"review\", \"type\"]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_encoder_2 = LabelEncoder()\n",
    "df.type = class_encoder_2.fit_transform(df.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "puttern = re.compile(\"^[a-z]+$\")\n",
    "def preprocess(text):\n",
    "    text = text.lower().strip()\n",
    "    words = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text) if (puttern.search(word)) and (word not in stopwords)]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review = df.review.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.review\n",
    "y = df.type\n",
    "n_classes = y.nunique()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, data):\n",
    "        self.max_seq_len = data.apply(lambda x: len(x)).max()\n",
    "        tokens = set()\n",
    "        for item in data:\n",
    "            tokens.update(item)\n",
    "        tokens = list(tokens)\n",
    "        tokens.insert(0, \"<PAD>\")\n",
    "        tokens.insert(1, \"<UNK>\")\n",
    "        self.idx_to_token = dict(enumerate(tokens))\n",
    "        self.token_to_idx = {token: idx for idx, token in self.idx_to_token.items()}\n",
    "        self.vocab_len = len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, y, vocab: Vocab):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        '''Генерирует представление отзыва'''\n",
    "        review_t = torch.zeros(self.vocab.max_seq_len).type(torch.long)\n",
    "        for i, token in enumerate(review):\n",
    "            try:\n",
    "                review_t[i] = self.vocab.token_to_idx[token]\n",
    "            except KeyError as ke:\n",
    "                review_t[i] = self.vocab.token_to_idx[\"<UNK>\"]\n",
    "        return review_t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.vectorize(self.X.iloc[idx]), self.y.iloc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Обучите классификатор.\n",
    "  \n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding` \n",
    "    - подберите адекватную размерность вектора эмбеддинга: \n",
    "    - модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(X_train)\n",
    "train_dataset = ReviewsDataset(X_train, y_train, vocab)\n",
    "test_dataset = ReviewsDataset(X_test, y_test, vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOwnClassifier(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, dropout_level=0.5):\n",
    "        super(MyOwnClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv1d(embedding_dim, 256, kernel_size=5),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(2),\n",
    "        )\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Conv1d(256, 512, kernel_size=3),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.MaxPool1d(2),\n",
    "        )\n",
    "#         self.l3 = nn.Sequential(\n",
    "#             nn.Conv1d(256, 512, kernel_size=3),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.MaxPool1d(2),\n",
    "#         )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_level),\n",
    "            nn.Linear(6*512, 1024),\n",
    "#             nn.Softmax(dim=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "#         self.fc2 = nn.Sequential(\n",
    "#             nn.Dropout(p=dropout_level),\n",
    "#             nn.Linear(1024, 256),\n",
    "# #             nn.Softmax(dim=1),\n",
    "#             nn.ReLU(),  \n",
    "#         )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_level),\n",
    "            nn.Linear(1024, n_classes),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, training=False):\n",
    "        x = self.embedding(x).transpose(1,2)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "#         x = self.l3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyOwnClassifier(num_embeddings=vocab.vocab_len, embedding_dim=128, dropout_level=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Training loss: 0.7622 training_acc: 0.4992 val_loss: 0.7590 val_acc: 0.4965\n",
      "#5 Training loss: 0.7501 training_acc: 0.5083 val_loss: 0.7603 val_acc: 0.4876\n",
      "#10 Training loss: 0.7292 training_acc: 0.5186 val_loss: 0.7365 val_acc: 0.5152\n",
      "#15 Training loss: 0.7196 training_acc: 0.5256 val_loss: 0.7289 val_acc: 0.5077\n",
      "#20 Training loss: 0.6861 training_acc: 0.5762 val_loss: 0.7172 val_acc: 0.5401\n",
      "#25 Training loss: 0.6463 training_acc: 0.6349 val_loss: 0.7124 val_acc: 0.5504\n",
      "#30 Training loss: 0.6093 training_acc: 0.6783 val_loss: 0.7082 val_acc: 0.5640\n",
      "#35 Training loss: 0.5614 training_acc: 0.7358 val_loss: 0.7045 val_acc: 0.5799\n",
      "#40 Training loss: 0.5113 training_acc: 0.7933 val_loss: 0.7027 val_acc: 0.5813\n",
      "#45 Training loss: 0.4677 training_acc: 0.8398 val_loss: 0.7031 val_acc: 0.5879\n",
      "#50 Training loss: 0.4386 training_acc: 0.8695 val_loss: 0.7032 val_acc: 0.5813\n",
      "#55 Training loss: 0.4050 training_acc: 0.9050 val_loss: 0.6941 val_acc: 0.6001\n",
      "#60 Training loss: 0.3856 training_acc: 0.9267 val_loss: 0.7018 val_acc: 0.5921\n",
      "#65 Training loss: 0.3755 training_acc: 0.9357 val_loss: 0.6971 val_acc: 0.5959\n",
      "#70 Training loss: 0.3634 training_acc: 0.9488 val_loss: 0.6932 val_acc: 0.6081\n",
      "#75 Training loss: 0.3577 training_acc: 0.9553 val_loss: 0.6977 val_acc: 0.6038\n",
      "#80 Training loss: 0.3497 training_acc: 0.9632 val_loss: 0.6964 val_acc: 0.6043\n",
      "#85 Training loss: 0.3461 training_acc: 0.9669 val_loss: 0.6916 val_acc: 0.6062\n",
      "#90 Training loss: 0.3421 training_acc: 0.9712 val_loss: 0.6932 val_acc: 0.6099\n",
      "#95 Training loss: 0.3388 training_acc: 0.9744 val_loss: 0.6909 val_acc: 0.6095\n",
      "#100 Training loss: 0.3383 training_acc: 0.9747 val_loss: 0.7023 val_acc: 0.6020\n"
     ]
    }
   ],
   "source": [
    "hard_training(model, criterion, optimizer, n_epochs=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.47      0.54      1067\n",
      "         1.0       0.58      0.72      0.64      1066\n",
      "\n",
      "    accuracy                           0.59      2133\n",
      "   macro avg       0.60      0.59      0.59      2133\n",
      "weighted avg       0.60      0.59      0.59      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GloP_boB(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
    "* Целевое значение accuracy на валидации - 70+%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_review(review):\n",
    "    x = train_dataset.vectorize(review).unsqueeze(0)\n",
    "    variety, predictions = model(x).topk(k=1, dim=1)\n",
    "    variety = variety.cpu().detach().view(-1)\n",
    "    pred_ = predictions.cpu().detach().view(-1)\n",
    "    pred_ = class_encoder_2.inverse_transform(pred_)\n",
    "    out_ = \", \".join([f\"{nat}:{frac:.2f}\" for nat, frac in zip(pred_, variety)])\n",
    "    print(f\"{review} --- {out_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ = [\n",
    "    \"The last time I had lunch here, I really liked the soup. Excellent. I'll come again.\",\n",
    "    \"It feels like I dined on frogs. I will not visit this institution again.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last', 'time', 'lunch', 'really', 'liked', 'soup', 'excellent', 'come'] --- negative:1.00\n",
      "['feel', 'like', 'dined', 'frog', 'visit', 'institution'] --- positive:1.00\n"
     ]
    }
   ],
   "source": [
    "for review in reviews_:\n",
    "     test_review(preprocess(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "#лол все наоборот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
